"""
LangGraph Tutorial Configuration Manager
æ”¯æŒå¤šç§LLMæä¾›å•†å’Œè‡ªå®šä¹‰é…ç½®çš„é…ç½®ç®¡ç†å™¨
"""

import os
import logging
from typing import Optional, Dict, Any, Union
from dataclasses import dataclass
from pathlib import Path
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.language_models import BaseChatModel


@dataclass
class LLMConfig:
    """LLMé…ç½®æ•°æ®ç±»"""
    provider: str
    api_key: str
    base_url: Optional[str] = None
    model: str = "gpt-4o-mini"
    temperature: float = 0.1
    max_tokens: int = 4000
    api_version: Optional[str] = None
    deployment_name: Optional[str] = None


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - æ”¯æŒå¤šç§LLMæä¾›å•†"""
    
    def __init__(self, env_file: Optional[str] = None):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            env_file: ç¯å¢ƒå˜é‡æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º .env
        """
        self.env_file = env_file or ".env"
        self._load_environment()
        self._setup_logging()
        
    def _load_environment(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        if os.path.exists(self.env_file):
            load_dotenv(self.env_file)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.env_file}")
        else:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ {self.env_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_level = os.getenv("LOG_LEVEL", "INFO")
        log_format = os.getenv(
            "LOG_FORMAT",
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )

        # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆåœ¨åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ä¹‹å‰ï¼‰
        Path("logs").mkdir(exist_ok=True)

        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format=log_format,
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler("logs/langgraph.log", encoding="utf-8")
            ]
        )
    
    def get_llm_config(self, provider: Optional[str] = None) -> LLMConfig:
        """
        è·å–LLMé…ç½®
        
        Args:
            provider: LLMæä¾›å•† (openai|anthropic|azure|custom)
            
        Returns:
            LLMConfig: LLMé…ç½®å¯¹è±¡
        """
        provider = provider or os.getenv("DEFAULT_LLM_PROVIDER", "openai")
        
        if provider == "openai":
            return self._get_openai_config()
        elif provider == "anthropic":
            return self._get_anthropic_config()
        elif provider == "azure":
            return self._get_azure_config()
        elif provider == "custom":
            return self._get_custom_config()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
    
    def _get_openai_config(self) -> LLMConfig:
        """è·å–OpenAIé…ç½®"""
        return LLMConfig(
            provider="openai",
            api_key=os.getenv("OPENAI_API_KEY", ""),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=float(os.getenv("OPENAI_TEMPERATURE", "0.1")),
            max_tokens=int(os.getenv("OPENAI_MAX_TOKENS", "4000"))
        )
    
    def _get_anthropic_config(self) -> LLMConfig:
        """è·å–Anthropicé…ç½®"""
        return LLMConfig(
            provider="anthropic",
            api_key=os.getenv("ANTHROPIC_API_KEY", ""),
            base_url=os.getenv("ANTHROPIC_BASE_URL", "https://api.anthropic.com"),
            model=os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307"),
            temperature=float(os.getenv("ANTHROPIC_TEMPERATURE", "0.1")),
            max_tokens=int(os.getenv("ANTHROPIC_MAX_TOKENS", "4000"))
        )
    
    def _get_azure_config(self) -> LLMConfig:
        """è·å–Azure OpenAIé…ç½®"""
        return LLMConfig(
            provider="azure",
            api_key=os.getenv("AZURE_OPENAI_API_KEY", ""),
            base_url=os.getenv("AZURE_OPENAI_ENDPOINT", ""),
            model=os.getenv("AZURE_MODEL", "gpt-4o-mini"),
            temperature=float(os.getenv("AZURE_OPENAI_TEMPERATURE", "0.1")),
            max_tokens=int(os.getenv("AZURE_OPENAI_MAX_TOKENS", "4000")),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01"),
            deployment_name=os.getenv("AZURE_DEPLOYMENT_NAME", "")
        )
    
    def _get_custom_config(self) -> LLMConfig:
        """è·å–è‡ªå®šä¹‰LLMé…ç½®"""
        return LLMConfig(
            provider="custom",
            api_key=os.getenv("CUSTOM_LLM_API_KEY", ""),
            base_url=os.getenv("CUSTOM_LLM_BASE_URL", ""),
            model=os.getenv("CUSTOM_LLM_MODEL", ""),
            temperature=float(os.getenv("CUSTOM_LLM_TEMPERATURE", "0.1")),
            max_tokens=int(os.getenv("CUSTOM_LLM_MAX_TOKENS", "4000"))
        )
    
    def create_llm(self, provider: Optional[str] = None, **kwargs) -> BaseChatModel:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            provider: LLMæä¾›å•†
            **kwargs: é¢å¤–çš„é…ç½®å‚æ•°
            
        Returns:
            BaseChatModel: LLMå®ä¾‹
        """
        config = self.get_llm_config(provider)
        
        # åˆå¹¶kwargsåˆ°é…ç½®ä¸­
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
        
        if not config.api_key:
            raise ValueError(f"ç¼ºå°‘ {config.provider.upper()} API Key")
        
        if config.provider == "openai":
            return ChatOpenAI(
                api_key=config.api_key,
                base_url=config.base_url,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        elif config.provider == "anthropic":
            return ChatAnthropic(
                api_key=config.api_key,
                base_url=config.base_url,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        elif config.provider == "azure":
            return ChatOpenAI(
                api_key=config.api_key,
                azure_endpoint=config.base_url,
                azure_deployment=config.deployment_name,
                api_version=config.api_version,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        elif config.provider == "custom":
            # è‡ªå®šä¹‰LLMï¼Œä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            return ChatOpenAI(
                api_key=config.api_key,
                base_url=config.base_url,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {config.provider}")
    
    def get_database_config(self) -> Dict[str, str]:
        """è·å–æ•°æ®åº“é…ç½®"""
        return {
            "database_url": os.getenv("DATABASE_URL", "sqlite:///data/langgraph_tutorial.db"),
            "checkpoint_db_url": os.getenv("CHECKPOINT_DB_URL", "sqlite:///data/checkpoints.db")
        }
    
    def get_workflow_config(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œæµé…ç½®"""
        return {
            "max_iterations": int(os.getenv("MAX_ITERATIONS", "10")),
            "default_timeout": int(os.getenv("DEFAULT_TIMEOUT", "300")),
            "enable_human_feedback": os.getenv("ENABLE_HUMAN_FEEDBACK", "true").lower() == "true"
        }
    
    def get_debug_config(self) -> Dict[str, Any]:
        """è·å–è°ƒè¯•é…ç½®"""
        return {
            "debug_mode": os.getenv("DEBUG_MODE", "false").lower() == "true",
            "show_graph_visualization": os.getenv("SHOW_GRAPH_VISUALIZATION", "true").lower() == "true",
            "save_intermediate_states": os.getenv("SAVE_INTERMEDIATE_STATES", "true").lower() == "true"
        }
    
    def validate_config(self, provider: Optional[str] = None) -> bool:
        """
        éªŒè¯é…ç½®æ˜¯å¦å®Œæ•´
        
        Args:
            provider: LLMæä¾›å•†
            
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            config = self.get_llm_config(provider)
            
            if not config.api_key:
                print(f"âŒ ç¼ºå°‘ {config.provider.upper()} API Key")
                return False
            
            if config.provider == "azure" and not config.deployment_name:
                print("âŒ Azure OpenAI ç¼ºå°‘ deployment_name")
                return False
            
            if config.provider == "custom" and not config.base_url:
                print("âŒ è‡ªå®šä¹‰LLM ç¼ºå°‘ base_url")
                return False
            
            print(f"âœ… {config.provider.upper()} é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“‹ LangGraph é…ç½®æ‘˜è¦")
        print("="*50)
        
        # LLMé…ç½®
        provider = os.getenv("DEFAULT_LLM_PROVIDER", "openai")
        config = self.get_llm_config(provider)
        
        print(f"ğŸ¤– LLMæä¾›å•†: {config.provider.upper()}")
        print(f"ğŸ“¦ æ¨¡å‹: {config.model}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦: {config.temperature}")
        print(f"ğŸ“ æœ€å¤§Token: {config.max_tokens}")
        
        if config.base_url:
            print(f"ğŸ”— APIåœ°å€: {config.base_url}")
        
        # æ•°æ®åº“é…ç½®
        db_config = self.get_database_config()
        print(f"ğŸ’¾ æ•°æ®åº“: {db_config['database_url']}")
        
        # å·¥ä½œæµé…ç½®
        workflow_config = self.get_workflow_config()
        print(f"ğŸ”„ æœ€å¤§è¿­ä»£: {workflow_config['max_iterations']}")
        print(f"ğŸ‘¤ äººå·¥åé¦ˆ: {'å¯ç”¨' if workflow_config['enable_human_feedback'] else 'ç¦ç”¨'}")
        
        # è°ƒè¯•é…ç½®
        debug_config = self.get_debug_config()
        print(f"ğŸ› è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if debug_config['debug_mode'] else 'ç¦ç”¨'}")
        
        print("="*50)


# å…¨å±€é…ç½®å®ä¾‹
config_manager = ConfigManager()


def get_llm(provider: Optional[str] = None, **kwargs) -> BaseChatModel:
    """
    è·å–LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        provider: LLMæä¾›å•†
        **kwargs: é¢å¤–é…ç½®å‚æ•°
        
    Returns:
        BaseChatModel: LLMå®ä¾‹
    """
    return config_manager.create_llm(provider, **kwargs)


if __name__ == "__main__":
    # é…ç½®æµ‹è¯•
    config_manager.print_config_summary()
    
    # éªŒè¯é…ç½®
    if config_manager.validate_config():
        print("\nâœ… é…ç½®éªŒè¯æˆåŠŸï¼")
        
        # åˆ›å»ºLLMå®ä¾‹æµ‹è¯•
        try:
            llm = config_manager.create_llm()
            print(f"ğŸ‰ æˆåŠŸåˆ›å»ºLLMå®ä¾‹: {type(llm).__name__}")
        except Exception as e:
            print(f"âŒ åˆ›å»ºLLMå®ä¾‹å¤±è´¥: {e}")
    else:
        print("\nâŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼") 